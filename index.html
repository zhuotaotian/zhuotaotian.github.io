<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
    <link rel="shortcut icon" href="myIcon.ico">
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="keywords" content="Li Jiang, The Chinese University of Hong Kong">
    <meta name="description" content="Li Jiang's home page">
    <link rel="stylesheet" href="jemdoc.css" type="text/css" />
    <title>Li Jiang</title>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39824124-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script');
        ga.type = 'text/javascript';
        ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(ga, s);
    })();
    </script>
</head>

<body>
<div id="layout-content" style="margin-top:25px">
<table>
    <tbody>
        <tr>
        	<td width="670">
                <div id="toptitle">
                    <h1>Li Jiang</h1>
				</div>
				<h3>
					Assistant Professor </br>
					Presidential Young Fellow
				</h3>
				<p>
					Room 610, Teaching Complex C </br>
					School of Data Science </br>
					The Chinese University of Hong Kong (Shenzhen) </br>
					Shenzhen, China</br>
					</br>
					Email: <a href="mailto:lijiangcse@gmail.com">lijiangcse [at] gmail [dot] com</a> or <a href="mailto:jiangli@cuhk.edu.cn">jiangli [at] cuhk [dot] edu [dot] cn</a>
				</p>
				<p>
					<a href="https://github.com/llijiang"><img src="pics/github.png" height="30px"></a>
					<a href="https://scholar.google.com/citations?user=5cIodxsAAAAJ&hl=en"><img src="pics/google_scholar.png" height="30px"></a>
					<!-- <a href=""><img src="pics/linkedin.png" height="30px"></a> -->
				</p>
			</td>
			<td><img src="pics/lijiang.jpg" border="0" height="250"></br></td>
		<tr>
	</tbody>
</table>

<h2>Biography</h2>
<p>
	<div style="text-align:justify">
	    I am an assistant professor in <a href="https://sds.cuhk.edu.cn/en">the School of Data Science</a>,
	    <a href="https://www.cuhk.edu.cn/en">Chinese University of Hong Kong, Shenzhen (CUHK-SZ)</a>.
	    Previously, I had a wonderful time as a postdoctoral researcher in <a href="https://www.mpi-inf.mpg.de/home/">Max Planck Institute for Informatics (MPI-INF)</a>,
		working with <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele">Prof. Bernt Schiele</a>. 
		I obtained my Ph.D. degree in <a href="http://www.cse.cuhk.edu.hk">Computer Science and Engineering Department</a>, 
		<a href="http://www.cuhk.edu.hk">The Chinese University of Hong Kong (CUHK)</a>, 
		supervised by <a href="http://www.cse.cuhk.edu.hk/~leojia">Prof. Jiaya Jia</a> and <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Prof. Chi-Wing Fu</a>. 
		Before that, I received the B.E. degree from <a href = "http://www.hit.edu.cn">Harbin Institute of Technology (HIT)</a> in 2017. 
		I was selected for the Excellent Young Scientists Fund Program (Overseas)（国家级高层次青年人才）.
	</div>
</p>

<p><font color="red">I am looking for self-motivated PhD and MPhil students (2025 & 2026 fall), posdoctoral researchers, and research assistant. If you are interested in working with me, please drop me an email with your resume.</font></p>

<p>My research interest includes computer vision and artificial intelligence. Our current research interest and focus include 3D scene understanding, autonomous driving & embodied AI, world modeling, generative modeling, representation learning, and multi-modal learning. </p>

<h2>Publications [<a href="https://scholar.google.com/citations?user=5cIodxsAAAAJ&hl=en">Google Scholar</a>]</h2>
<ul>
	* indicates equal contribution; † indicates corresponding author.
	<!-- <li>
		<a href=""></a><br>
		.<br>
		<em></em> <br>
		<p style="margin-top:3px">
			[<a href="">Paper</a>]
			[<a href="">Code</a>]
			[<a href="">Bib</a>]
		</p>
	</li> -->
	<li>
		<a href="https://arxiv.org/abs/2505.19239">DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving</a><br>
		Chen Shi, Shaoshuai Shi, Kehua Sheng, Bo Zhang, <b>Li Jiang†</b>.<br>
		<em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2025.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2505.19239">Paper</a>]
			<!-- [<a href="">Code</a>] -->
			[<a href="papers/iccv25_drivex_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2506.10507">Edit360: 2D Image Edits to 3D Assets from Any Angle</a><br>
		Junchao Huang, Xinting Hu, Shaoshuai Shi, Zhuotao Tian, <b>Li Jiang†</b>.<br>
		<em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2025.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2506.10507">Paper</a>]
			[<a href="https://junchao-cs.github.io/Edit360-demo/">Project</a>]
			<!-- [<a href="">Code</a>] -->
			[<a href="papers/iccv25_edit360_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2506.23120">Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation</a><br>
		Zhenhua Ning, Zhuotao Tian, Shaoshuai Shi, Guangming Lu, Daojing He, Wenjie Pei†, <b>Li Jiang†</b>.<br>
		<em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2025.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2506.23120">Paper</a>]
			<!-- [<a href="">Code</a>] -->
			[<a href="papers/iccv25_sr_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="">Mitigating Object Hallucinations via Sentence-Level Early Intervention</a><br>
		Shangpin Peng, Senqiao Yang, <b>Li Jiang</b>, Zhuotao Tian.<br>
		<em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2025.</br>
		<p style="margin-top:3px">
			[<a href="">Paper</a>]
			<!-- [<a href="">Code</a>] -->
			[<a href="papers/iccv25_mitigating_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2501.01163">3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer</a><br>
		Jiajun Deng, Tianyu He, <b>Li Jiang</b>, Tianyu Wang, Feras Dayoub, Ian Reid.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2025.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2501.01163">Paper</a>]
			<!-- [<a href="">Code</a>] -->
			[<a href="papers/cvpr25_3dllava_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2503.08422">JiSAM: Alleviate Labeling Burden and Corner Case Problems in Autonomous Driving via Minimal Real-World Data</a><br>
		Runjian Chen, Wenqi Shao, Bo Zhang, Shaoshuai Shi, <b>Li Jiang</b>, Ping Luo.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2025.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2503.08422">Paper</a>]
			<!-- [<a href="">Code</a>] -->
			[<a href="papers/cvpr25_jisam_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/html/2309.10230v3">LiON: Learning Point-wise Abstaining Penalty for LiDAR Outlier DetectioN Using Diverse Synthetic Data</a><br>
		Shaocong Xu, Pengfei Li, Qianpu Sun, Xinyu Liu, Yang Li, Shihui Guo, Zhen Wang, Bo Jiang, Rui Wang, Kehua Sheng, Bo Zhang, <b>Li Jiang</b>, Hao Zhao, Yilun Chen.<br>
		<em>AAAI Conference on Artificial Intelligence</em> (<b>AAAI</b>), 2025.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/html/2309.10230v3">Paper</a>]
			[<a href="https://github.com/Daniellli/LiON/">Code</a>]
			[<a href="papers/aaai25_lion_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2403.09394">GiT: Towards Generalist Vision Transformer through Universal Language Interface</a><br>
		Haiyang Wang*, Hao Tang*, <b>Li Jiang†</b>, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, Liwei Wang†.<br>
		<em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2024.<br>
		<b><font color="red">Oral Presentation.</font></b><br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2403.09394">Paper</a>]
			[<a href="https://github.com/Haiyang-W/GiT">Code</a>]
			[<a href="papers/eccv24_git_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2407.21654">MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment</a><br>
		Anurag Das, Xinting Hu, <b>Li Jiang</b>, Bernt Schiele.<br>
		<em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2024.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2407.21654">Paper</a>]
			<!-- [<a href="">Code</a>] -->
			[<a href="papers/eccv24_mtaclip_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://storage.googleapis.com/waymo-uploads/files/research/2024%20Technical%20Reports/2024%20WOD%20Motion%20Prediction%20Challenge%20-%201st%20Place%20-%20MTR%20v3.pdf">MTR v3: 1st Place Solution for 2024 Waymo Open Dataset Challenge - Motion Prediction</a><br>
		Chen Shi, Shaoshuai Shi, <b>Li Jiang†</b>.<br>
		<em>Waymo Motion Prediction Challenge at Workshop on Autonomous Driving of CVPR</em> (<b>CVPRW</b>), 2024. <br>
		<font color="red">Won the champion of Waymo Open Dataset Motion Prediction Challenge 2024 (June 2024).</font>
		<p style="margin-top:3px">
			[<a href="https://storage.googleapis.com/waymo-uploads/files/research/2024%20Technical%20Reports/2024%20WOD%20Motion%20Prediction%20Challenge%20-%201st%20Place%20-%20MTR%20v3.pdf">Paper</a>]
			<!-- [<a href="">Code</a>] -->
			[<a href="papers/cvprw24_mtrv3_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2312.10035">Point Transformer V3: Simpler, Faster, Stronger</a><br>
		Xiaoyang Wu, <b>Li Jiang</b>, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, Hengshuang Zhao.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024.<br>
		<b><font color="red">Oral Presentation (0.78% acceptance rate).</font></b><br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2312.10035">Paper</a>]
			[<a href="https://github.com/Pointcept/PointTransformerV3">Code</a>]
			[<a href="papers/cvpr24_pointtransformerv3_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_GroupContrast_Semantic-aware_Self-supervised_Representation_Learning_for_3D_Understanding_CVPR_2024_paper.pdf">GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding</a><br>
		Chengyao Wang, <b>Li Jiang</b>, Xiaoyang Wu, Zhuotao Tian, Bohao Peng, Hengshuang Zhao, Jiaya Jia.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024.<br>
		<p style="margin-top:3px">
			[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_GroupContrast_Semantic-aware_Self-supervised_Representation_Learning_for_3D_Understanding_CVPR_2024_paper.pdf">Paper</a>]
			[<a href="https://github.com/dvlab-research/GroupContrast">Code</a>]
			[<a href="papers/cvpr24_groupcontrast_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Training_Vision_Transformers_for_Semi-Supervised_Semantic_Segmentation_CVPR_2024_paper.pdf">Training Vision Transformers for Semi-Supervised Semantic Segmentation</a><br>
		Xinting Hu, <b>Li Jiang</b>, Bernt Schiele.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024.<br>
		<p style="margin-top:3px">
			[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Training_Vision_Transformers_for_Semi-Supervised_Semantic_Segmentation_CVPR_2024_paper.pdf">Paper</a>]
			<!-- [<a href="https://github.com/JoyHuYY1412/S4Former">Code</a>] -->
			[<a href="papers/cvpr24_s4former_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_OA-CNNs_Omni-Adaptive_Sparse_CNNs_for_3D_Semantic_Segmentation_CVPR_2024_paper.pdf">Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation</a><br>
		Bohao Peng, Xiaoyang Wu, <b>Li Jiang†</b>, Yukang Chen, Hengshuang Zhao, Zhuotao Tian†, Jiaya Jia.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024.<br>
		<p style="margin-top:3px">
			[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_OA-CNNs_Omni-Adaptive_Sparse_CNNs_for_3D_Semantic_Segmentation_CVPR_2024_paper.pdf">Paper</a>]
			[<a href="https://github.com/Pointcept/Pointcept">Code</a>]
			[<a href="papers/cvpr24_oacnns_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2404.07155">Unified Language-driven Zero-shot Domain Adaptation</a><br>
		Senqiao Yang, Zhuotao Tian, <b>Li Jiang</b>, Jiaya Jia.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2404.07155">Paper</a>]
			[<a href="https://senqiaoyang.com/project/ulda/">Project</a>]
			[<a href="papers/cvpr24_ulda_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2305.06973">FreePoint: Unsupervised Point Cloud Instance Segmentation</a><br>
		Zhikai Zhang, Jian Ding, <b>Li Jiang</b>, Dengxin Dai, Gui-Song Xia.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2305.06973">Paper</a>]
			<!-- [<a href="https://github.com/zzk273/FreePoint">Code</a>] -->
			[<a href="papers/cvpr24_freepoint_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Open-Vocabulary_3D_Semantic_Segmentation_with_Foundation_Models_CVPR_2024_paper.pdf">Open-Vocabulary 3D Semantic Segmentation with Foundation Models</a><br>
		<b>Li Jiang</b>, Shaoshuai Shi, Bernt Schiele.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024.<br>
		<b><font color="red">Highlight (2.8% acceptance rate).</font></b><br>
		<p style="margin-top:3px">
			[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Open-Vocabulary_3D_Semantic_Segmentation_with_Foundation_Models_CVPR_2024_paper.pdf">Paper</a>]
			[<a href="papers/cvpr24_ov3d_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2306.17770">MTR++: Multi-Agent Motion Prediction with Symmetric Scene Modeling and Guided Intention Querying</a><br>
		Shaoshuai Shi*, <b>Li Jiang*†</b>, Dengxin Dai, Bernt Schiele.<br>
		<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (<b>T-PAMI</b>), accepted, 2024.<br>
		<font color="red">Won the champion of Waymo Open Dataset Motion Prediction Challenge 2023 (May 2023).</font><br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2306.17770">Paper</a>]
			[<a href="papers/tpami24_mtr++_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2305.05026">Self-supervised Pre-training with Masked Shape Prediction for 3D Scene Understanding</a><br>
		<b>Li Jiang</b>, Zetong Yang, Shaoshuai Shi, Vladislav Golyanik, Dengxin Dai, Bernt Schiele.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2023.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2305.05026">Paper</a>]
			[<a href="papers/cvpr23_msp_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2303.11633">Learning Context-aware Classifier for Semantic Segmentation</a><br>
		Zhuotao Tian, Jiequan Cui, <b>Li Jiang</b>, Xiaojuan Qi, Xin Lai, Yixin Chen, Shu Liu, Jiaya Jia.<br>
		<em>AAAI Conference on Artificial Intelligence</em> (<b>AAAI</b>), 2023.<br>
		<b><font color="red">Oral Presentation.</font></b><br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2303.11633">Paper</a>]
			[<a href="https://github.com/tianzhuotao/CAC">Code</a>]
			[<a href="papers/aaai23_contextaware_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2210.05666">Point Transformer V2: Grouped Vector Attention and Partition-based Pooling</a><br>
		Xiaoyang Wu, Yixing Lao, <b>Li Jiang</b>, Xihui Liu, Hengshuang Zhao.<br>
		<em>Conference on Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2022.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2210.05666">Paper</a>]
			[<a href="https://github.com/Gofinge/PointTransformerV2">Code</a>]
			[<a href="papers/neurips22_ptv2_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2209.13508">Motion Transformer with Global Intention Localization and Local Movement Refinement</a><br>
		Shaoshuai Shi, <b>Li Jiang</b>, Dengxin Dai, Bernt Schiele.<br>
		<em>Conference on Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2022.<br>
		<b><font color="red">Oral Presentation.</font></b><br>
		<font color="red">Ranked 1st on Waymo motion prediction and interaction prediction two leaderboards (May 2022).</font><br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2209.13508">Paper</a>]
			[<a href="https://github.com/sshaoshuai/MTR">Code</a>]
			[<a href="papers/neurips22_motion_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2209.10033">MTR-A: 1st Place Solution for 2022 Waymo Open Dataset Challenge–Motion Prediction</a><br>
		Shaoshuai Shi, <b>Li Jiang</b>, Dengxin Dai, Bernt Schiele.<br>
		<em>Waymo Motion Prediction Challenge at Workshop on Autonomous Driving of CVPR</em> (<b>CVPRW</b>), 2022. <br>
		<font color="red">Won the champion of Waymo Open Dataset Motion Prediction Challenge 2022 (June 2022).</font>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2209.10033">Paper</a>]
			[<a href="https://github.com/sshaoshuai/MTR">Code</a>]
			[<a href="papers/cvprw22_mtra_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2102.00463">PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection</a><br>
		Shaoshuai Shi, <b>Li Jiang</b>, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, Hongsheng Li.<br>
		<em>International Journal of Computer Vision</em> (<b>IJCV</b>), accepted, 2022. <br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2102.00463">Paper</a>]
			[<a href="https://github.com/open-mmlab/OpenPCDet">Code</a>]
			[<a href="papers/ijcv22_pv_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2207.01030">Boosting Single-Frame 3D Object Detection by Simulating Multi-Frame Point Clouds</a><br>
		Wu Zheng, <b>Li Jiang</b>, Fanbin Lu, Yangyang Ye, Chi-Wing Fu.<br>
		<em>ACM International Conference on Multimedia</em> (<b>ACMMM</b>), 2022.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2207.01030">Paper</a>]
			[<a href="papers/acmmm22_boosting_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2204.01599">DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation</a><br>
		Runyu Ding*, Jihan Yang*, <b>Li Jiang</b>, Xiaojuan Qi. (*: equal contribution)<br>
		<em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2022.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2204.01599">Paper</a>]
			[<a href="https://github.com/CVMI-Lab/DODA">Code</a>]
			[<a href="papers/eccv22_doda_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/pdf/2203.01252.pdf">A Unified Query-based Paradigm for Point Cloud Understanding</a><br>
		Zetong Yang*, <b>Li Jiang*</b>, Yanan Sun, Bernt Schiele, Jiaya Jia. (*: equal contribution)<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2022.<br>
		<p style="margin-top:3px">
			[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_A_Unified_Query-Based_Paradigm_for_Point_Cloud_Understanding_CVPR_2022_paper.pdf">Paper</a>]
			[<a href="https://github.com/dvlab-research/DeepVision3D">Code</a>]
			[<a href="papers/cvpr22_eq_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Boosting_3D_Object_Detection_by_Simulating_Multimodality_on_Point_Clouds_CVPR_2022_paper.pdf">Boosting 3D Object Detection by Simulating Multimodality on Point Clouds</a><br>
		Wu Zheng, Mingxuan Hong, <b>Li Jiang</b>, Chi-Wing Fu.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2022.<br>
		<b><font color="red">Oral Presentation.</font></b><br>
		<p style="margin-top:3px">
			[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Boosting_3D_Object_Detection_by_Simulating_Multimodality_on_Point_Clouds_CVPR_2022_paper.pdf">Paper</a>]
			[<a href="papers/cvpr22_boosting_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tian_Generalized_Few-Shot_Semantic_Segmentation_CVPR_2022_paper.pdf">Generalized Few-Shot Semantic Segmentation</a><br>
		Zhuotao Tian, Xin Lai, <b>Li Jiang</b>, Michelle Shu, Hengshuang Zhao, Jiaya Jia.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2022.<br>
		<p style="margin-top:3px">
			[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tian_Generalized_Few-Shot_Semantic_Segmentation_CVPR_2022_paper.pdf">Paper</a>]
			[<a href="https://github.com/dvlab-research/GFS-Seg">Code</a>]
			[<a href="papers/cvpr22_gfs_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2203.14508">Stratified Transformer for 3D Point Cloud Segmentation</a><br>
		Xin Lai*, Jianhui Liu*, <b>Li Jiang</b>, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, Jiaya Jia. (*: equal contribution)<br>
		<em>Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2022.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2203.14508">Paper</a>]
			[<a href="https://github.com/dvlab-research/Stratified-Transformer">Code</a>]
			[<a href="papers/cvpr22_stratified_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://ieeexplore.ieee.org/document/9736597">Adaptive Perspective Distillation for Semantic Segmentation</a><br>
		Zhuotao Tian, Pengguang Chen, Xin Lai, <b>Li Jiang</b>, Shu Liu, Hengshuang Zhao, Bei Yu, Ming-Chang Yang, Jiaya Jia.<br>
		<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (<b>T-PAMI</b>), accepted, 2022.<br>
		<p style="margin-top:3px">
			[<a href="https://ieeexplore.ieee.org/document/9736597">Paper</a>]
			<!-- [<a href="">Code</a>] -->
			[<a href="papers/tpami22_apd_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/pdf/2110.08188.pdf">Guided Point Contrastive Learning for Semi-supervised Point Cloud Semantic Segmentation</a><br>
		<b>Li Jiang</b>, Shaoshuai Shi, Zhuotao Tian, Xin Lai, Shu Liu, Chi-Wing Fu, Jiaya Jia.<br>
		<em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2021.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/pdf/2110.08188.pdf">Paper</a>]
			[<a href="https://github.com/llijiang/GuidedContrast">Code</a>]
			[<a href="papers/iccv21_guidedcontrast_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2012.09164">Point Transformer</a><br>
		Hengshuang Zhao, <b>Li Jiang</b>, Jiaya Jia, Philip Torr, Vladlen Koltun.<br>
		<em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2021.</br>
		<b><font color="red">Oral Presentation (3.4% acceptance rate).</font></b><br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2012.09164">Paper</a>]
			[<a href="papers/iccv21_pointtransformer_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2106.14133">Semi-supervised Semantic Segmentation with Directional Context-aware Consistency</a><br>
		Xin Lai*, Zhuotao Tian*, <b>Li Jiang</b>, Shu Liu, Hengshuang Zhao, Liwei Wang, Jiaya Jia. (*: equal contribution)<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2021.</br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2106.14133">Paper</a>]
			[<a href="https://github.com/dvlab-research/Context-Aware-Consistency">Code</a>]
			[<a href="papers/cvpr21_cac_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2103.14326">Bidirectional Projection Network for Cross Dimension Scene Understanding</a><br>
		Wenbo Hu*, Hengshuang Zhao*, <b>Li Jiang</b>, Jiaya Jia, Tien-Tsin Wong. (*: equal contribution)<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2021.</br>
		<b><font color="red">Oral Presentation (4.3% acceptance rate).</font></b><br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2103.14326">Paper</a>]
			[<a href="https://github.com/wbhu/BPNet">Code</a>]
			[<a href="papers/cvpr21_bpnet_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2104.09804">SE-SSD: Self-Ensembling Single-Stage Object Detector From Point Cloud</a><br>
		Wu Zheng, Weiliang Tang, <b>Li Jiang</b>, Chi-Wing Fu.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2021.</br>
		<font color="red">Ranked 1st/2nd on KITTI BEV/3D object detection benchmark (Car, Nov 2020 - May 2021).</font><br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2104.09804">Paper</a>]
			[<a href="https://github.com/Vegeta2020/SE-SSD">Code</a>]
			[<a href="papers/cvpr21_sessd_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://arxiv.org/abs/2012.03015">CIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point Cloud</a><br>
		Wu Zheng, Weiliang Tang, Sijin Chen, <b>Li Jiang</b>, Chi-Wing Fu.<br>
		<em>AAAI Conference on Artificial Intelligence</em> (<b>AAAI</b>), 2021.<br>
		<p style="margin-top:3px">
			[<a href="https://arxiv.org/abs/2012.03015">Paper</a>]
			[<a href="https://github.com/Vegeta2020/CIA-SSD">Code</a>]
			[<a href="papers/aaai21_ciassd_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_PointGroup_Dual-Set_Point_Grouping_for_3D_Instance_Segmentation_CVPR_2020_paper.pdf">PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation</a><br>
		<b>Li Jiang*</b>, Hengshuang Zhao*, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya Jia. (*: equal contribution)<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2020.</br>
		<b><font color="red">Oral Presentation (5.7% acceptance rate).</font></b><br>
		<font color="red">Ranked 1st on ScanNet 3D Semantic instance benchmark (Nov-16 2019).</font><br>
		<p style="margin-top:3px">
			[<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_PointGroup_Dual-Set_Point_Grouping_for_3D_Instance_Segmentation_CVPR_2020_paper.pdf">Paper</a>]
			[<a href="https://github.com/dvlab-research/PointGroup">Code</a>]
			[<a href="papers/cvpr20_pointgroup_bib.txt">Bib</a>]
		</p>
		<b></b>
	</li>
	<li>
		<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_PV-RCNN_Point-Voxel_Feature_Set_Abstraction_for_3D_Object_Detection_CVPR_2020_paper.pdf">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</a><br>
		Shaoshuai Shi, Chaoxu Guo, <b>Li Jiang</b>, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2020.</br>
		<font color="red">Ranked 1st on KITTI 3D object detection benchmark (Car, Nov 2019 - Aug 2020).</font><br>
		<p style="margin-top:3px">
			[<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_PV-RCNN_Point-Voxel_Feature_Set_Abstraction_for_3D_Object_Detection_CVPR_2020_paper.pdf">Paper</a>]
			[<a href="https://github.com/sshaoshuai/PV-RCNN">Code</a>]
			[<a href="papers/cvpr20_pvrcnn_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Hierarchical_Point-Edge_Interaction_Network_for_Point_Cloud_Semantic_Segmentation_ICCV_2019_paper.pdf">Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation</a><br>
		<b>Li Jiang</b>, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-Wing Fu, Jiaya Jia.<br>
		<em>IEEE International Conference on Computer Vision</em> (<b>ICCV</b>), 2019.</br>
		<p style="margin-top:3px">
			[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Hierarchical_Point-Edge_Interaction_Network_for_Point_Cloud_Semantic_Segmentation_ICCV_2019_paper.pdf">Paper</a>]
			[<a href="papers/iccv19_pointedge_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Qi_Amodal_Instance_Segmentation_With_KINS_Dataset_CVPR_2019_paper.pdf">Amodal Instance Segmentation through KINS Dataset</a><br>
		Lu Qi, <b>Li Jiang</b>, Shu Liu, Xiaoyong Shen, Jiaya Jia.<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2019.</br>
		<p style="margin-top:3px">
			[<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Qi_Amodal_Instance_Segmentation_With_KINS_Dataset_CVPR_2019_paper.pdf">Paper</a>]
			[<a href="https://github.com/qqlu/Amodal-Instance-Segmentation-through-KINS-Dataset">Code</a>]
			[<a href="papers/cvpr19_amodal_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="papers/cvpr19_pointweb.pdf">PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing</a><br>
		Hengshuang Zhao*, <b>Li Jiang*</b>, Chi-Wing Fu, Jiaya Jia. (*: equal contribution)<br>
		<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2019.</br>
		<p style="margin-top:3px">
			[<a href="papers/cvpr19_pointweb.pdf">Paper</a>]
			[<a href="https://github.com/hszhao/PointWeb">Code</a>]
			[<a href="https://youtu.be/CaobqpsUP_4">Video</a>]
			[<a href="papers/cvpr19_pointweb_bib.txt">Bib</a>]
		</p>
	</li>
	<li>
		<a href="https://link.springer.com/chapter/10.1007/978-3-030-01237-3_49">GAL: Geometric Adversarial Loss for Single-View 3D-Object Reconstruction</a><br>
		<b>Li Jiang</b>, Shaoshuai Shi, Xiaojuan Qi, Jiaya Jia.<br>
		<em>European Conference on Computer Vision</em> (<b>ECCV</b>), 2018.<br>
		<b><font color="red">Oral Presentation (2.1% acceptance rate).</font></b><br>
		<p style="margin-top:3px">
			[<a href="https://link.springer.com/chapter/10.1007/978-3-030-01237-3_49">Paper</a>]
			[<a href="papers/eccv18_gal_bib.txt">Bib</a>]
		</p>
	</li>
</ul>

<h2>Experience</h2>
<ul>
	<li>
		<div style="float:left; text-align:left"><a href="https://www.ox.ac.uk/">Oxford University</a>, Oxford, England</div> <div style="float:right; text-align:right">Jan 2021 – Jul 2021</div><br>
		Visitor (Remote)<br>
		Advisor: <a href="https://hszhao.github.io/">Hengshuang Zhao</a> and <a href="">Philip Torr</a><br>
		Topic: Hand Parsing and Reconstruction<br>
	</li>
	<li>
		<div style="float:left; text-align:left"><a href="https://smartmore.global/">SmartMore</a>, Shenzhen, China</div> <div style="float:right; text-align:right">Feb 2020 – Jul 2021</div><br>
		Computer Vision Research Intern<br>
		Advisor: <a href="http://shuliu.me/">Shu Liu</a><br>
		Topic: 3D Scene Understanding, Robotic Grasping<br>
	</li>
	<li>
		<div style="float:left; text-align:left"><a href="https://www.tencent.com/en-us/">Tencent YouTu Lab</a>, Shenzhen, China</div> <div style="float:right; text-align:right">Jul 2018 – Jan 2020</div><br>
		Computer Vision Research Intern<br>
		Advisor: <a href="http://shuliu.me/">Shu Liu</a> and <a href="">Xiaoyong Shen</a><br>
		Topic: 3D Semantic Segmentation, Depth Prediction<br>
	</li>
	<li>
		<div style="float:left; text-align:left"><a href="https://www.anu.edu.au/">Australian National University (ANU)</a>, Canberra, Australia</div> <div style="float:right; text-align:right">Jul 2016 - Dec 2016</div><br>
		Exchange Student<br>
		Topic: Document Analysis<br>
	</li>
</ul>

<h2>Professional Activities</h2>
<ul>
	<li>Conference Reviewer:<br>
		&emsp; IEEE Conference on Computer Vision and Pattern Recognition (CVPR).<br>
		&emsp; IEEE International Conference on Computer Vision (ICCV).<br>
		&emsp; European Conference on Computer Vision (ECCV).<br>
		&emsp; Neural Information Processing Systems (NeurIPS).<br>
		&emsp; International Conference on Learning Representations (ICLR).<br>
		&emsp; International Conference on Machine Learning (ICML).<br>
		&emsp; ACM Multimedia (ACM-MM).<br>
		&emsp; SIGGRAPH. <br>
	</li>
	<li>Journal Reviewer:<br>
		&emsp; IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).<br>
		&emsp; International Journal of Computer Vision (IJCV).<br>
		&emsp; IEEE Transactions on Image Processing (TIP).<br>
		&emsp; Pattern Recognition (PR).<br>
	</li>
</ul>

<h2>Honors & Awards</h2>
<table style="border-spacing:2px" width="100%">
<tbody>
	<tr><td>World’s Top 2% Scientist (Single Year), Elsevier</td><td>2024</td></tr>
	<tr><td>Champion of <a href="https://waymo.com/open/challenges/">Waymo Open Dataset Motion Prediction challenge</a></td><td>2024</td></tr>
	<tr><td>Champion of <a href="https://waymo.com/open/challenges/">Waymo Open Dataset Motion Prediction challenge</a></td><td>2023</td></tr>
	<tr><td>Champion of <a href="https://waymo.com/open/challenges/">Waymo Open Dataset Motion Prediction challenge</a></td><td>2022</td></tr>
	<tr><td>Lise Meitner Postdoctoral Fellowship</td><td>2021-2022</td></tr>
	<tr><td>ECCV Outstanding Reviewer Award</td><td>2020</td></tr>
	<tr><td><a href="https://cerg1.ugc.edu.hk/hkpfs/index.html">Hong Kong Ph.D. Fellowship</a> (The highest scholarship for PhD students in Hong Kong)</td> <td>2017-2021</td></tr>
	<tr><td>Outstanding Graduate Award in Harbin Institute of Technology</td><td>2017</td></tr>
	<tr><td>Provincial-Level Merit Student of Hei Longjiang Province</td><td>2016</td></tr>
	<tr><td>National Scholarship</td><td>2015, 2016</td></tr>
	<tr><td>Fuji Xerox’s Scholarship</td><td>2014</td></tr>
	<tr><td>First Class People’s Scholarship</td><td>2013-2017</td></tr>
</tbody>
</table>

<h2>Teaching</h2>
<ul>
	<li>Teaching:
		<table id="Teaching" border="0" width="100%">
			<tbody>
				<tr>
					<td style="width:80%">CSC3100 Data Structure</td><td>Spring</td><td>2023-2024</td>
				</tr>
				<tr>
					<td style="width:80%">CSC6051/MDS5112 Image Processing and Computer Vision</td><td>Fall</td><td>2024-2025</td>
				</tr>
				<tr>
					<td style="width:80%">CSC3100 Data Structure</td><td>Spring</td><td>2024-2025</td>
				</tr>
			</tbody>
		</table>
	</li>

	<li>Teaching Assistant:
		<table id="TeachingAssistant" border="0" width="100%">
			<tbody>
				<tr>
					<td style="width:80%">ENGG5104 Image Processing and Computer Vision</td><td>Spring</td><td>2020-2021</td>
				</tr>
				<tr>
					<td>ENGG1110 Problem Solving By Programming</td><td>Fall</td><td>2019-2020</td>
				</tr>
				<tr>
					<td>CSCI2100B Data Structure</td><td>Spring</td><td>2018-2019</td>
				</tr>
				<tr>
					<td>ENGG1110 Problem Solving By Programming</td><td>Fall</td><td>2018-2019</td>
				</tr>
				<tr>
					<td>ENGG1100 Introduction to Engineering Design</td><td>Spring</td><td>2017-2018</td>
				</tr>
				<tr>
					<td>CSCI1130 Introduction to Computing Using JAVA</td><td>Fall</td><td>2017-2018</td>
				</tr>
			</tbody>
		</table>
	</li>
</ul>

<!-- <div id="footer"> -->
	<!-- <div id="footer-text"></div>
	</div>
		<center>© Li Jiang | 2022</center>
	</div> -->

	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-88615920-1', 'auto');
	ga('send', 'pageview');

	</script>
<!-- </div> -->

<div id="cm" style="display: none;">
	<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=EJGgXVNElphSoGp99mBvvA0EUD4cBLxcWb-YyZQBjro&cl=ffffff&w=a"></script>
</div>

</body>
</html>
